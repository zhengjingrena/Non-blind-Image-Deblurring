
function net = Init_LEARN_Model


lr  = [1 1];
weightDecay = [1 0];

% Define network
net.layers = {} ;

 for i = 1:1:50
 net.layers{end+1} = struct('type', 'dzdysum');
net.layers{end+1} = struct('type', 'CTmapping','weights',{{0.1}}, 'learningRate',lr, 'weightDecay',weightDecay); 
net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{0.01*randn(5,5,3,48,'single'), zeros(48,1,'single')}}, ...
    'stride', 1, ...
    'pad', 2, ...
    'learningRate',lr, ...
    'weightDecay',weightDecay, ...
    'opts',{{}}) ;
net.layers{end+1} = struct('type', 'relu','leak',0) ;
net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{0.01*randn(5,5,48,48,'single'), zeros(48,1,'single')}}, ...
    'stride', 1, ...
    'pad', 2, ...
    'learningRate',lr, ...
    'weightDecay',weightDecay, ...
    'opts',{{}}) ;
net.layers{end+1} = struct('type', 'relu','leak',0) ;

net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{0.01*randn(5,5,48,3,'single'), zeros(3,1,'single')}}, ...
    'stride', 1, ...
    'pad', 2, ...
    'learningRate',lr, ...
    'weightDecay',weightDecay, ...
    'opts',{{}}) ;

net.layers{end+1} = struct('type', 'sum');
net.layers{end+1} = struct('type', 'relu','leak',0) ;
 end

net.layers{end+1} = struct('type', 'loss') ; % make sure the new 'vl_nnloss.m' is in the same folder.
net = vl_simplenn_tidy(net);
